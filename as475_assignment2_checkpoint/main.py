# -*- coding: utf-8 -*-
"""IMLProject2 final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UsxC-OkDh8hnQz2evFODHQCsifJFJGTE
"""

#IML Project 2
import tensorflow as tf
tf.keras.datasets.cifar10
from keras.datasets import cifar10
from matplotlib import pyplot as plt
import numpy as np
import random as rd
from matplotlib.pyplot import cm
from sklearn.metrics import silhouette_samples,silhouette_score
from sklearn.preprocessing import StandardScaler
from validclust import ValidClust
from validclust import dunn
from sklearn.metrics import pairwise_distances


#Load Data from Cifer10 dataset  and flatting the images for further use.
def load_data():
    (Train_X, Train_Y), (Test_X, Test_Y) = cifar10.load_data()
    Train_X = Train_X.astype('float32') / 255
    Test_X = Test_X.astype('float32') / 255 
    #Flattening the images
    Train_X = Train_X.reshape((-1, 3072))
    Test_X = Test_X.reshape((-1, 3072))
    
    return Test_X

# Kmean method to assign the value of clusters and iteration.
def Kmean():
    clusters=10
    iteration=100 

    return clusters, iteration

#Update method to find the distances between centroids and data point.
def update_clusters(Train_X, centroids):
  clus_group = []
  distances= []
  #SSE = []
  
  for r in Train_X:
    sum =0
    for centroid in centroids:
      distances.append(np.sqrt(np.dot(r-centroid,r-centroid)))
    less_distance = min(distances)
    index_pos = distances.index(less_distance)
    clus_group.append(index_pos)
    distances.clear()
    
    
  return np.array(clus_group)

#Change centroid is used to find the new centroids after calculating the mean of datapoints
def change_centroids(Train_X, clus_group):
  n_centroids=[]
  cluster_no=np.unique(clus_group)

  for no in cluster_no:
    n_centroids.append(Train_X[clus_group == no].mean(axis=0))
    
  return np.array(n_centroids)

#Predict method is used to assign the first centroids and calling update clusters and change centroids, then calculate silhouette_score and Dunn Index.
def predict():
  Train_X=load_data()
  n_clusters, iteration=Kmean()
  #rd_index=rd.sample(range(0,Train_X.shape[0]),n_clusters)
  #print(rd_index)
  #centroids=Train_X[rd_index]
  centroids=Train_X[[8509, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 8999]]
  
  for i in range(iteration):
    clus_group=update_clusters(Train_X, centroids)
    old_centroids=centroids
    centroids=change_centroids(Train_X, clus_group)
    if(old_centroids==centroids).all():
      break
  
  print("silhouette_score")
  print(silhouette_score(Train_X, clus_group))
  dist = pairwise_distances(Train_X)
  print("Dunn Index")
  print(dunn(dist, clus_group))

predict()

